{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e3847f",
   "metadata": {},
   "source": [
    "# 有关pytorch自动求导机制的探索与展示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a597f",
   "metadata": {},
   "source": [
    "pytorch的自动求导机制是一个强大的功能，本人（笨人）在此尝试边学边写一个文件用来展示，有不正确的地方欢迎骂我菜就多练并联系我\n",
    "[指正](mailto:12410615@mail.sustech.edu.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891fc08",
   "metadata": {},
   "source": [
    "## 叶子节点与非叶子节点\n",
    "\n",
    "pytorch的操作围绕张量展开，而pytorch的张量又可以分为两种，一种是叶子张量，一种是非叶子张量。\n",
    "叶子张量，简单理解，就是某个枝干的尽头，它代表某个“end”，叶子会连在枝干上，但不会有东西连在叶子上。在实际操作中，叶子张量就是指用户直接创建的张量，而不是通过某些函数操作生成的张量。\n",
    "\n",
    "非叶子张量，与叶子张量相对，它是有迹可循的，是被叶子连接的枝干，一个不太恰当的解释是，非叶子张量是叶子张量通过一些函数操作得到的中间张量（这种解释真的及其不恰当，但可以先这么简单理解）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0fc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 代码展示\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# 框框引入一堆包\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# 电脑上没有nvidia显卡，（默默流下眼泪），就用cpu来展示叭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b39484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0], requires_grad=True) # 这里创建一个tensor，它是一个源头，是我们直接手动创建的，应该是一个叶子节点\n",
    "print(a.is_leaf) # 检验一下，发现确实是一个叶子节点\n",
    "\n",
    "b = a * 2 # 某种函数操作\n",
    "print(b.is_leaf) # 发现b不是叶子节点，因为它是由a计算出来的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb35a45",
   "metadata": {},
   "source": [
    "## grad与grad_fn\n",
    "### grad\n",
    "为什么需要区分叶子张量与非叶子张量？我们可以思考一下我们的优化方法（梯度下降那一坨），我们的任务是优化我们的权重，调整权重的值来给出更好的输出。此处我们的权重就是我们首先初始化（直接创建，是叶子张量），然后用梯度调整的值，它需要grad(梯度)。而中间生成的张量，并不是我们需要在意的东西，简言之，我们只需要把梯度从非叶子张量中间传回去，而不需要保留非叶子张量的梯度，因为我们的终极任务是调整权重大小（叶子张量数值），非叶子张量不是我们的目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a54446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\w1369\\AppData\\Local\\Temp\\ipykernel_22888\\3387406872.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(b.grad) # 打印一下b的梯度，发现b的梯度是None，因为b不是叶子节点，我们并不在意，所以没有梯度\n"
     ]
    }
   ],
   "source": [
    "# 我们刚刚创建了a，b张量，这里拿来展示\n",
    "b.backward() # 反向传播，计算梯度\n",
    "print(a.grad) # 打印一下a的梯度，发现确实是2.0 (db/da = 2a = 2*1.0 = 2.0)\n",
    "print(b.grad) # 打印一下b的梯度，发现b的梯度是None，因为b不是叶子节点，我们并不在意，所以没有梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950eee0f",
   "metadata": {},
   "source": [
    "### grad_fn\n",
    "\n",
    "一个很有意思的事情，pytorch如何实现这种自动求导机制？我们只需要在张量上进行一个backward方法的调用，就可以算出之前的梯度值。\n",
    "\n",
    "答案是，pytorch会保存中间张量（非叶子张量）的生成操作，我们刚才的非叶子张量b是通过a*2得到的，所以pytorch会自动记录这种操作，在backward反向传播的时候，会读取这种操作的反向传播方式，然后把梯度回传回去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<MulBackward0 object at 0x0000020B0B418430>\n"
     ]
    }
   ],
   "source": [
    "print(a.grad_fn) # 打印一下a的grad_fn，发现是None，因为a是一个叶子节点\n",
    "print(b.grad_fn) # 打印一下b的grad_fn，发现是一个MulBackward对象，说明b是由a计算出来的，这里MulBackward代表b是a乘出来的\n",
    "# backward反向传播时，为了把b的梯度传递给a，PyTorch会读取这种操作的梯度反向传递方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3aa0f",
   "metadata": {},
   "source": [
    "但是如果为了一些其他的操作，这里我们也可以保留b的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed3be0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "tensor([1.])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "b.retain_grad() # 让b保留梯度\n",
    "b.backward() # 反向传播，计算梯度\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "# 芜湖，我们得到了和之前不一样的结果，db/db = 1.0，b的梯度在这里也成功的保留在了grad里边，但是注意，b仍然是非叶子节点。\n",
    "print(b.is_leaf) # 仍然是非叶子节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9cc9f",
   "metadata": {},
   "source": [
    "## detach\n",
    "一个很有意思的操作是detach操作，它的操作原理是\"创建一个新的 Tensor，从当前计算图中分离出来，新的 Tensor 不具备梯度信息。\"\n",
    "(截取自grok)\n",
    "我们可以进一步的思考这会带来什么效果，这个张量会被创建出来，然后它理应是一个新的叶子节点，而此处由于grad_fn会指向None，我们\n",
    "将无法把梯度回传给上游，下边我们来验证一下我们的猜想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2de06bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<AddBackward0 object at 0x0000020B23FEC850>\n",
      "True\n",
      "None\n",
      "None\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "c = torch.tensor([1.0], requires_grad=True) \n",
    "# 一口气创建三个叶子张量\n",
    "d = a + b + c\n",
    "# 这里创建了一个新的张量d，它是由a、b、c计算出来的\n",
    "e = d\n",
    "# 我们把d的引用传给e，现在e是由一个等式操作得到的非叶子张量，我们验证一下我们的猜想\n",
    "print(e.is_leaf) # False,发现e不是叶子节点\n",
    "print(e.grad_fn) # 存储了生成e的function操作信息\n",
    "\n",
    "# 接下来我们给它上压力，detach一下\n",
    "e = e.detach()\n",
    "# 现在我们把e给截断，从原有计算图中分离出来\n",
    "print(e.is_leaf) # True,发现e是叶子节点\n",
    "print(e.grad_fn) # 发现e的grad_fn是None，因为它是一个叶子节点\n",
    "e.requires_grad_() # 让e重新需要梯度\n",
    "h = e * 2\n",
    "# 既然e已经从原有的计算图中截取出来，那么此处如果从h调用backward方法，那么理论上来讲梯度信息不会回传给abc，\n",
    "# 只能回传给e，我们验证一下猜想\n",
    "h.backward()\n",
    "print(a.grad) # None,发现a的梯度是None\n",
    "print(e.grad) # 发现e的梯度是1.0，说明h的梯度确实回传给了e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda18bf",
   "metadata": {},
   "source": [
    "## requires_grad?\n",
    "刚刚其实我一直有意的回避requires_grad这个属性的影响，因为我们可以想想，对于模型的训练阶段，我们会需要保留梯度信息，以便进行梯度下降调整权重，但是在模型的推理阶段，这种梯度就是多余的操作，我们并不需要它。也就是说，总会存在不同的场景，我们的叶子张量需要梯度或不需要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c68acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "None\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0], requires_grad=False) # 默认情况下requires_grad其实也是False\n",
    "b = torch.tensor([1.0], requires_grad=False)\n",
    "c = a + b\n",
    "print(c.requires_grad) # False,发现c的requires_grad也是False\n",
    "# 生成c的两个张量不需要梯度，所以c也不需要梯度\n",
    "# 那如果b需要梯度呢？\n",
    "b.requires_grad_() # 让b需要梯度\n",
    "c = a + b\n",
    "print(c.requires_grad) # False,发现c的requires_grad变为了True\n",
    "# 这说明了一个问题，c的requires_grad是由它的输入决定的，而不是由它的输出决定的\n",
    "c.backward() # 反向传播，计算梯度\n",
    "print(a.grad) # None,发现a的梯度是None,(因为a的requires_grad是False)\n",
    "print(b.grad) # 发现b的梯度是1.0，说明c的梯度确实回传给了b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607be405",
   "metadata": {},
   "source": [
    "嗯，就写到这了，我太懒了。\n",
    "我是菜狗，欢迎说我菜就多练并指正！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
